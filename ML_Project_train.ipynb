{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c784375a-775f-469f-a8b6-78a14999e193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from decimal import *\n",
    "import math \n",
    "from statistics import mean\n",
    "import pickle\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2675af91-8e0b-4010-88c5-d29b17f63743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Datatypes structuring... \n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_behaviors = pd.read_parquet('ebnerd_demo/train/behaviors.parquet')\n",
    "train_history = pd.read_parquet('ebnerd_demo/train/history.parquet')\n",
    "val_behaviors = pd.read_parquet('ebnerd_demo/validation/behaviors.parquet')\n",
    "val_history = pd.read_parquet('ebnerd_demo/validation/history.parquet')\n",
    "articles = pd.read_parquet('ebnerd_demo/articles.parquet')\n",
    "\n",
    "# Converting train and validation history data to long format\n",
    "train_history_expanded = train_history.explode(['impression_time_fixed', 'scroll_percentage_fixed', 'article_id_fixed', 'read_time_fixed'])\n",
    "val_history_expanded = val_history.explode(['impression_time_fixed', 'scroll_percentage_fixed', 'article_id_fixed', 'read_time_fixed'])\n",
    "\n",
    "# Update column names\n",
    "train_history_expanded.rename(columns={'article_id_fixed': 'article_id', 'impression_time_fixed': 'impression_time', 'scroll_percentage_fixed': 'scroll_percentage', 'read_time_fixed': 'read_time'}, inplace=True)\n",
    "val_history_expanded.rename(columns={'article_id_fixed': 'article_id', 'impression_time_fixed': 'impression_time', 'scroll_percentage_fixed': 'scroll_percentage', 'read_time_fixed': 'read_time'}, inplace=True)\n",
    "\n",
    "pd.options.mode.chained_assignment = None  \n",
    "\n",
    "print(\"Datatypes structuring... \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d29d7b0-984d-4836-84a9-1570d54defe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values ​​and encode string values ​​to float\n",
    "for i in range(len(articles['subcategory'])):\n",
    "    if (len(articles['subcategory'][i])==0):\n",
    "        articles['subcategory'][i] = [-1] \n",
    "    articles['subcategory'][i]=float(mean(articles['subcategory'][i])) #Reduce size by averaging the list\n",
    "for i in range(len(articles['image_ids'])):\n",
    "    if (type(articles['image_ids'][i]) is not np.ndarray):\n",
    "        articles['image_ids'][i] = [0] #It means there is no picture in the article\n",
    "    articles['image_ids'][i]=float(mean(articles['image_ids'][i])) #Reduce size by averaging the list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cef3cc91-441d-405c-9b0b-268db64fa85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "import math\n",
    "from decimal import Decimal\n",
    "\n",
    "def string_to_float32(s):\n",
    "    # Compute the hash of the string\n",
    "    hash_object = hashlib.sha256(s.encode('ascii', errors='backslashreplace'))\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    \n",
    "    # Convert hash to an integer\n",
    "    hash_int = int(hash_hex, 16)\n",
    "    \n",
    "    # Normalize the hash to a float32 range (0, 1)\n",
    "    max_hash_int = int('f' * len(hash_hex), 16)\n",
    "    normalized = hash_int / max_hash_int\n",
    "    \n",
    "    # Convert normalized value to float32\n",
    "    return np.float32(normalized)\n",
    "\n",
    "for column in ['ner_clusters', 'entity_groups', 'topics']:\n",
    "    data = articles[column]\n",
    "    for i in range(len(data)):\n",
    "        string_data_combined = \"\"\n",
    "        for j in range(len(data[i])):\n",
    "            string_data_combined += \"/\" + data[i][j]  # All strings in the list are concatenated into a single string\n",
    "        \n",
    "        new = string_to_float32(string_data_combined)\n",
    "        data[i] = new\n",
    "    articles[column] = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "203dc690-90bb-4673-b031-4f8ae2a42561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "import math\n",
    "from decimal import Decimal\n",
    "\n",
    "def string_to_float32(s):\n",
    "    # Compute the hash of the string\n",
    "    hash_object = hashlib.sha256(s.encode('ascii', errors='backslashreplace'))\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    \n",
    "    # Convert hash to an integer\n",
    "    hash_int = int(hash_hex, 16)\n",
    "    \n",
    "    # Normalize the hash to a float32 range (0, 1)\n",
    "    max_hash_int = int('f' * len(hash_hex), 16)\n",
    "    normalized = hash_int / max_hash_int\n",
    "    \n",
    "    # Convert normalized value to float32\n",
    "    return np.float32(normalized)\n",
    "\n",
    "for column in ['title', 'subtitle']:\n",
    "    data = articles[column].astype(str)\n",
    "    for i in range(len(data)):\n",
    "        new = data[i]\n",
    "        new = string_to_float32(new)\n",
    "        data[i] = new\n",
    "    articles[column] = data\n",
    "\n",
    "# Coding categorical variables for train and validation behaviors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoders = {}\n",
    "for column in ['device_type', 'gender']:\n",
    "    le = LabelEncoder()\n",
    "    train_behaviors[column] = le.fit_transform(train_behaviors[column].astype(str))\n",
    "    val_behaviors[column] = le.transform(val_behaviors[column].astype(str))\n",
    "    label_encoders[column] = le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "772ebc1e-83dc-4035-9006-92f7862bee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coding categorical variables for the Articles dataset\n",
    "for column in ['article_type', 'category_str', 'sentiment_label']:\n",
    "    le = LabelEncoder()\n",
    "    articles[column] = le.fit_transform(articles[column].astype(str))\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Normalize numeric variables\n",
    "scaler = StandardScaler()\n",
    "articles[['sentiment_score']] = scaler.fit_transform(articles[['sentiment_score']].astype(float)) #The number value for Sentimen Score is already low so there is no reshaping.\n",
    "articles[['total_read_time']] = scaler.fit_transform(articles[['total_read_time']].astype(float).values.reshape(-1, 1))\n",
    "articles[['total_pageviews']] = scaler.fit_transform(articles[['total_pageviews']].astype(float).values.reshape(-1, 1))\n",
    "articles[['total_inviews']] = scaler.fit_transform(articles[['total_inviews']].astype(float).values.reshape(-1, 1))\n",
    "articles[['title']] = scaler.fit_transform(articles[['title']].astype(float).values.reshape(-1, 1))\n",
    "articles[['subtitle']] = scaler.fit_transform(articles[['subtitle']].astype(float).values.reshape(-1, 1))\n",
    "articles[['ner_clusters']] = scaler.fit_transform(articles[['ner_clusters']].astype(float).values.reshape(-1, 1))\n",
    "articles[['entity_groups']] = scaler.fit_transform(articles[['entity_groups']].astype(float).values.reshape(-1, 1))\n",
    "articles[['topics']] = scaler.fit_transform(articles[['topics']].astype(float).values.reshape(-1, 1))\n",
    "articles[['subcategory']] = scaler.fit_transform(articles[['subcategory']].astype(float).values.reshape(-1, 1))\n",
    "articles[['category']] = scaler.fit_transform(articles[['category']].astype(float).values.reshape(-1, 1))\n",
    "articles[['image_ids']] = scaler.fit_transform(articles[['image_ids']].astype(float).values.reshape(-1, 1)) #There may be values ​​that can be further normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27466a2a-b06e-45b7-9817-5b2420e04439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling date and time variables\n",
    "train_behaviors['impression_time'] = pd.to_datetime(train_behaviors['impression_time']) ##Only hour and day are taken from dates\n",
    "train_behaviors['impression_hour'] = train_behaviors['impression_time'].dt.hour\n",
    "train_behaviors['impression_day'] = train_behaviors['impression_time'].dt.day\n",
    "val_behaviors['impression_time'] = pd.to_datetime(val_behaviors['impression_time'])\n",
    "val_behaviors['impression_hour'] = val_behaviors['impression_time'].dt.hour\n",
    "val_behaviors['impression_day'] = val_behaviors['impression_time'].dt.day\n",
    "\n",
    "train_history_expanded['impression_time'] = pd.to_datetime(train_history_expanded['impression_time'])\n",
    "train_history_expanded['impression_hour'] = train_history_expanded['impression_time'].dt.hour\n",
    "train_history_expanded['impression_day'] = train_history_expanded['impression_time'].dt.day\n",
    "val_history_expanded['impression_time'] = pd.to_datetime(val_history_expanded['impression_time'])\n",
    "val_history_expanded['impression_hour'] = val_history_expanded['impression_time'].dt.hour\n",
    "val_history_expanded['impression_day'] = val_history_expanded['impression_time'].dt.day\n",
    "\n",
    "articles['last_modified_time'] = pd.to_datetime(articles['last_modified_time'])\n",
    "articles['last_modified_hour'] = articles['last_modified_time'].dt.hour\n",
    "articles['last_modified_day'] = articles['last_modified_time'].dt.day\n",
    "\n",
    "articles['published_time'] = pd.to_datetime(articles['published_time'])\n",
    "articles['published_time_hour'] = articles['published_time'].dt.hour\n",
    "articles['published_time_day'] = articles['published_time'].dt.day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e497429d-d28e-46b3-bf74-94015028f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unnecessary columns\n",
    "train_behaviors.drop(['impression_time'], axis=1, inplace=True)\n",
    "train_behaviors.drop(['article_id'], axis=1, inplace=True)\n",
    "train_behaviors.drop(['next_read_time'], axis=1, inplace=True)\n",
    "train_behaviors.drop(['next_scroll_percentage'], axis=1, inplace=True)\n",
    "#train_behaviors.drop(['impression_id'], axis=1, inplace=True)\n",
    "train_behaviors.drop(['session_id'], axis=1, inplace=True)\n",
    "\n",
    "val_behaviors.drop(['impression_time'], axis=1, inplace=True)\n",
    "val_behaviors.drop(['article_id'], axis=1, inplace=True)\n",
    "val_behaviors.drop(['next_read_time'], axis=1, inplace=True)\n",
    "val_behaviors.drop(['next_scroll_percentage'], axis=1, inplace=True)\n",
    "#val_behaviors.drop(['impression_id'], axis=1, inplace=True)\n",
    "val_behaviors.drop(['session_id'], axis=1, inplace=True)\n",
    "\n",
    "train_history_expanded.drop(['impression_time'], axis=1, inplace=True)\n",
    "val_history_expanded.drop(['impression_time'], axis=1, inplace=True)\n",
    "\n",
    "articles.drop(['last_modified_time'], axis=1, inplace=True)\n",
    "articles.drop(['published_time'], axis=1, inplace=True)\n",
    "articles.drop(['body'], axis=1, inplace=True)\n",
    "articles.drop(['url'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bcd4b51-d133-4ff6-a315-c2401a1c1849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Train History Expanded columns: Index(['user_id', 'scroll_percentage', 'article_id', 'read_time',\n",
      "       'impression_hour', 'impression_day'],\n",
      "      dtype='object')\n",
      "Validation History Expanded columns: Index(['user_id', 'scroll_percentage', 'article_id', 'read_time',\n",
      "       'impression_hour', 'impression_day'],\n",
      "      dtype='object')\n",
      "Train Behaviors Expanded columns: Index(['impression_id', 'read_time', 'scroll_percentage', 'device_type',\n",
      "       'article_ids_inview', 'article_ids_clicked', 'user_id', 'is_sso_user',\n",
      "       'gender', 'postcode', 'age', 'is_subscriber', 'impression_hour',\n",
      "       'impression_day'],\n",
      "      dtype='object')\n",
      "Validation Behaviors Expanded columns: Index(['impression_id', 'read_time', 'scroll_percentage', 'device_type',\n",
      "       'article_ids_inview', 'article_ids_clicked', 'user_id', 'is_sso_user',\n",
      "       'gender', 'postcode', 'age', 'is_subscriber', 'impression_hour',\n",
      "       'impression_day'],\n",
      "      dtype='object')\n",
      "Articles columns: Index(['article_id', 'title', 'subtitle', 'premium', 'image_ids',\n",
      "       'article_type', 'ner_clusters', 'entity_groups', 'topics', 'category',\n",
      "       'subcategory', 'category_str', 'total_inviews', 'total_pageviews',\n",
      "       'total_read_time', 'sentiment_score', 'sentiment_label',\n",
      "       'last_modified_hour', 'last_modified_day', 'published_time_hour',\n",
      "       'published_time_day'],\n",
      "      dtype='object')\n",
      "Dataset Merging... \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Let's check the column names\n",
    "print(\"-------------------------------------------\") \n",
    "print(\"Train History Expanded columns:\", train_history_expanded.columns)\n",
    "print(\"Validation History Expanded columns:\", val_history_expanded.columns)\n",
    "\n",
    "print(\"Train Behaviors Expanded columns:\", train_behaviors.columns)\n",
    "print(\"Validation Behaviors Expanded columns:\", val_behaviors.columns)\n",
    "\n",
    "print(\"Articles columns:\", articles.columns)\n",
    "\n",
    "print(\"Dataset Merging... \")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9453092-1574-417d-a2fd-46bdaaff6a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_behaviors_expanded = train_behaviors.explode(['article_ids_inview']) #A record is created in the behaviors table for each reviewed article.\n",
    "val_behaviors_expanded = val_behaviors.explode(['article_ids_inview'])\n",
    "train_behaviors_expanded[['article_ids_inview']] = train_behaviors_expanded[['article_ids_inview']].astype(int)\n",
    "val_behaviors_expanded[['article_ids_inview']] = val_behaviors_expanded[['article_ids_inview']].astype(int)\n",
    "\n",
    "train_behaviors_expanded = train_behaviors_expanded.explode(['article_ids_clicked']) #Univariate lists are being removed from the list\n",
    "val_behaviors_expanded = val_behaviors_expanded.explode(['article_ids_clicked'])\n",
    "train_behaviors_expanded[['article_ids_clicked']] = train_behaviors_expanded[['article_ids_clicked']].astype(int)\n",
    "val_behaviors_expanded[['article_ids_clicked']] = val_behaviors_expanded[['article_ids_clicked']].astype(int)\n",
    "\n",
    "articles[['article_id']] = articles[['article_id']].astype(int) #It is necessary to make comparisons\n",
    "\n",
    "train_behaviors_expanded[\"article_id\"] = train_behaviors_expanded[\"article_ids_inview\"] #To combine the behaviors and articles tables, a column with the same name article_id is required in both tables.\n",
    "train_behaviors_expanded.drop(['article_ids_inview'], axis=1, inplace=True)\n",
    "val_behaviors_expanded[\"article_id\"] = val_behaviors_expanded[\"article_ids_inview\"]\n",
    "val_behaviors_expanded.drop(['article_ids_inview'], axis=1, inplace=True)\n",
    "\n",
    "train_behaviors_final = train_behaviors_expanded.merge(articles, on='article_id', how='left') \n",
    "val_behaviors_final = val_behaviors_expanded.merge(articles, on='article_id', how='left')\n",
    "train_history_final = train_history_expanded.merge(articles, on='article_id', how='left') \n",
    "val_history_final = val_history_expanded.merge(articles, on='article_id', how='left')\n",
    "\n",
    "X_history_final = pd.concat([train_history_final, val_history_final])\n",
    "\n",
    "train_final_ids = pd.DataFrame()\n",
    "train_final_ids['impression_id'] = train_behaviors_final['impression_id']\n",
    "train_final_ids['article_id'] = train_behaviors_final['article_id']\n",
    "train_final_ids['user_id'] = train_behaviors_final['user_id']\n",
    "val_final_ids = pd.DataFrame()\n",
    "val_final_ids['impression_id'] = val_behaviors_final['impression_id']\n",
    "val_final_ids['article_id'] = val_behaviors_final['article_id']\n",
    "val_final_ids['user_id'] = val_behaviors_final['user_id']\n",
    "final_ids = pd.concat([train_final_ids, val_final_ids])\n",
    "final_ids = final_ids.reset_index()\n",
    "final_ids = final_ids.drop(['index'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7f37bd3-7fbe-48da-9cac-d4a7b9654209",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_behaviors_final['clicked'] = np.where((train_behaviors_final['article_id'] == train_behaviors_final['article_ids_clicked']), True, False) #Checking whether the displayed article is the same as the clicked article.\n",
    "val_behaviors_final['clicked'] = np.where((val_behaviors_final['article_id'] == val_behaviors_final['article_ids_clicked']), True, False)\n",
    "\n",
    "train_behaviors_final.drop(['impression_id'], axis=1, inplace=True)\n",
    "val_behaviors_final.drop(['impression_id'], axis=1, inplace=True)\n",
    "\n",
    "train_behaviors_final.drop(['article_id'], axis=1, inplace=True) ##There is no need for article IDs as there is now a 'clicked' column that shows the clicked articles\n",
    "val_behaviors_final.drop(['article_id'], axis=1, inplace=True)\n",
    "train_behaviors_final.drop(['article_ids_clicked'], axis=1, inplace=True)\n",
    "val_behaviors_final.drop(['article_ids_clicked'], axis=1, inplace=True)\n",
    "X_history_final.drop(['article_id'], axis=1, inplace=True)\n",
    "\n",
    "X_train_phase1 = train_behaviors_final.drop(['device_type','is_sso_user','gender','postcode','age','is_subscriber'], axis=1) \n",
    "Y_train_phase1 = train_behaviors_final.filter(['device_type','is_sso_user','gender','postcode','age','is_subscriber']) \n",
    "X_val_phase1 = val_behaviors_final.drop(['device_type','is_sso_user','gender','postcode','age','is_subscriber'], axis=1)\n",
    "Y_val_phase1 = val_behaviors_final.filter(['device_type','is_sso_user','gender','postcode','age','is_subscriber'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a5ec25f-814c-40ac-8dd4-498ac7656446",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_history_rearranged = pd.DataFrame() #The History table must be in the same order as the Behavior table, so the same columns are added to the new table in this order.\n",
    "X_history_rearranged['read_time'] = X_history_final['read_time']\n",
    "X_history_rearranged['scroll_percentage'] = X_history_final['scroll_percentage']\n",
    "X_history_rearranged['user_id'] = X_history_final['user_id']\n",
    "X_history_rearranged['impression_hour'] = X_history_final['impression_hour']\n",
    "X_history_rearranged['impression_day'] = X_history_final['impression_day']\n",
    "X_history_rearranged['title'] = X_history_final['title']\n",
    "X_history_rearranged['subtitle'] = X_history_final['subtitle']\n",
    "X_history_rearranged['premium'] = X_history_final['premium']\n",
    "X_history_rearranged['image_ids'] = X_history_final['image_ids']\n",
    "X_history_rearranged['article_type'] = X_history_final['article_type']\n",
    "X_history_rearranged['ner_clusters'] = X_history_final['ner_clusters']\n",
    "X_history_rearranged['entity_groups'] = X_history_final['entity_groups']\n",
    "X_history_rearranged['topics'] = X_history_final['topics']\n",
    "X_history_rearranged['category'] = X_history_final['category']\n",
    "X_history_rearranged['subcategory'] = X_history_final['subcategory']\n",
    "X_history_rearranged['category_str'] = X_history_final['category_str']\n",
    "X_history_rearranged['total_inviews'] = X_history_final['total_inviews']\n",
    "X_history_rearranged['total_pageviews'] = X_history_final['total_pageviews']\n",
    "X_history_rearranged['total_read_time'] = X_history_final['total_read_time']\n",
    "X_history_rearranged['sentiment_score'] = X_history_final['sentiment_score']\n",
    "X_history_rearranged['sentiment_label'] = X_history_final['sentiment_label']\n",
    "X_history_rearranged['last_modified_hour'] = X_history_final['last_modified_hour']\n",
    "X_history_rearranged['last_modified_day'] = X_history_final['last_modified_day']\n",
    "X_history_rearranged['published_time_hour'] = X_history_final['published_time_hour']\n",
    "X_history_rearranged['published_time_day'] = X_history_final['published_time_day']\n",
    "X_history_rearranged['clicked'] = True\n",
    "\n",
    "X_history_final = X_history_rearranged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12cda163-758f-481c-b860-2d4184e90281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "('read_time', 0           0.0\n",
      "1           0.0\n",
      "2           0.0\n",
      "3           0.0\n",
      "4           0.0\n",
      "          ...  \n",
      "281725      0.0\n",
      "281726      0.0\n",
      "281727      0.0\n",
      "281728    107.0\n",
      "281729      0.0\n",
      "Name: read_time, Length: 281730, dtype: float32)\n",
      "('scroll_percentage', 0           0.0\n",
      "1           0.0\n",
      "2           0.0\n",
      "3           0.0\n",
      "4           0.0\n",
      "          ...  \n",
      "281725      0.0\n",
      "281726      0.0\n",
      "281727      0.0\n",
      "281728    100.0\n",
      "281729      0.0\n",
      "Name: scroll_percentage, Length: 281730, dtype: float32)\n",
      "('user_id', 0           22779\n",
      "1           22779\n",
      "2           22779\n",
      "3           22779\n",
      "4           22779\n",
      "           ...   \n",
      "281725    2096611\n",
      "281726    2096611\n",
      "281727    2096611\n",
      "281728    2096611\n",
      "281729    2096611\n",
      "Name: user_id, Length: 281730, dtype: uint32)\n",
      "('impression_hour', 0         21\n",
      "1         21\n",
      "2         21\n",
      "3         21\n",
      "4         21\n",
      "          ..\n",
      "281725    10\n",
      "281726    10\n",
      "281727    10\n",
      "281728    10\n",
      "281729    10\n",
      "Name: impression_hour, Length: 281730, dtype: int32)\n",
      "('impression_day', 0         21\n",
      "1         21\n",
      "2         21\n",
      "3         21\n",
      "4         21\n",
      "          ..\n",
      "281725    18\n",
      "281726    18\n",
      "281727    18\n",
      "281728    18\n",
      "281729    18\n",
      "Name: impression_day, Length: 281730, dtype: int32)\n",
      "('title', 0        -0.263498\n",
      "1         0.464761\n",
      "2         0.535514\n",
      "3        -0.966801\n",
      "4        -1.549098\n",
      "            ...   \n",
      "281725   -0.621162\n",
      "281726   -0.376689\n",
      "281727   -1.595999\n",
      "281728    0.137328\n",
      "281729   -1.558234\n",
      "Name: title, Length: 281730, dtype: float64)\n",
      "('subtitle', 0        -1.455986\n",
      "1        -1.288847\n",
      "2         0.068493\n",
      "3        -1.697130\n",
      "4        -0.091296\n",
      "            ...   \n",
      "281725   -0.809950\n",
      "281726    0.300740\n",
      "281727   -1.485645\n",
      "281728    1.129733\n",
      "281729   -0.523472\n",
      "Name: subtitle, Length: 281730, dtype: float64)\n",
      "('premium', 0         False\n",
      "1         False\n",
      "2          True\n",
      "3         False\n",
      "4         False\n",
      "          ...  \n",
      "281725    False\n",
      "281726    False\n",
      "281727    False\n",
      "281728    False\n",
      "281729    False\n",
      "Name: premium, Length: 281730, dtype: bool)\n",
      "('image_ids', 0         0.478182\n",
      "1         0.476904\n",
      "2         0.304440\n",
      "3         0.478506\n",
      "4         0.464597\n",
      "            ...   \n",
      "281725    0.413643\n",
      "281726    0.354242\n",
      "281727    0.476986\n",
      "281728    0.425587\n",
      "281729    0.358765\n",
      "Name: image_ids, Length: 281730, dtype: float64)\n",
      "('article_type', 0         0\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         0\n",
      "         ..\n",
      "281725    0\n",
      "281726    0\n",
      "281727    0\n",
      "281728    0\n",
      "281729    0\n",
      "Name: article_type, Length: 281730, dtype: int32)\n",
      "('ner_clusters', 0        -1.649036\n",
      "1         1.148704\n",
      "2         0.688605\n",
      "3        -0.871113\n",
      "4         0.244122\n",
      "            ...   \n",
      "281725    1.274121\n",
      "281726   -1.141049\n",
      "281727   -1.122696\n",
      "281728    0.301072\n",
      "281729   -0.294236\n",
      "Name: ner_clusters, Length: 281730, dtype: float64)\n",
      "('entity_groups', 0         1.127174\n",
      "1        -1.550297\n",
      "2        -1.848623\n",
      "3         1.436111\n",
      "4         0.414356\n",
      "            ...   \n",
      "281725   -0.589413\n",
      "281726    0.730117\n",
      "281727   -1.720999\n",
      "281728    0.010168\n",
      "281729   -0.356596\n",
      "Name: entity_groups, Length: 281730, dtype: float64)\n",
      "('topics', 0        -1.265224\n",
      "1        -0.405480\n",
      "2         1.353613\n",
      "3        -0.532586\n",
      "4        -0.451126\n",
      "            ...   \n",
      "281725   -1.525028\n",
      "281726   -0.925508\n",
      "281727   -0.532586\n",
      "281728   -0.914210\n",
      "281729   -0.912661\n",
      "Name: topics, Length: 281730, dtype: float64)\n",
      "('category', 0        -0.045757\n",
      "1         0.018845\n",
      "2        -0.457407\n",
      "3        -0.454402\n",
      "4        -0.490459\n",
      "            ...   \n",
      "281725   -0.045757\n",
      "281726   -0.045757\n",
      "281727   -0.454402\n",
      "281728   -0.490459\n",
      "281729   -0.045757\n",
      "Name: category, Length: 281730, dtype: float64)\n",
      "('subcategory', 0        -0.000254\n",
      "1         3.158043\n",
      "2        -0.559279\n",
      "3        -0.297276\n",
      "4        -0.393258\n",
      "            ...   \n",
      "281725   -0.006739\n",
      "281726   -0.006739\n",
      "281727   -0.299871\n",
      "281728   -0.389366\n",
      "281729   -0.001551\n",
      "Name: subcategory, Length: 281730, dtype: float64)\n",
      "('category_str', 0         23\n",
      "1          5\n",
      "2          9\n",
      "3         22\n",
      "4         12\n",
      "          ..\n",
      "281725    23\n",
      "281726    23\n",
      "281727    22\n",
      "281728    12\n",
      "281729    23\n",
      "Name: category_str, Length: 281730, dtype: int32)\n",
      "('total_inviews', 0        -0.171284\n",
      "1        -0.543432\n",
      "2         4.203480\n",
      "3         0.344785\n",
      "4         0.138932\n",
      "            ...   \n",
      "281725   -0.158092\n",
      "281726   -0.371519\n",
      "281727   -0.041105\n",
      "281728    1.449051\n",
      "281729    0.647910\n",
      "Name: total_inviews, Length: 281730, dtype: float64)\n",
      "('total_pageviews', 0        -0.053736\n",
      "1        -0.779281\n",
      "2         0.788721\n",
      "3         0.918672\n",
      "4         0.354590\n",
      "            ...   \n",
      "281725   -0.234311\n",
      "281726   -0.220430\n",
      "281727   -0.043067\n",
      "281728    2.126350\n",
      "281729    0.661633\n",
      "Name: total_pageviews, Length: 281730, dtype: float64)\n",
      "('total_read_time', 0        -0.036755\n",
      "1        -0.640071\n",
      "2         0.925025\n",
      "3         0.699433\n",
      "4         0.509225\n",
      "            ...   \n",
      "281725    0.223054\n",
      "281726   -0.187580\n",
      "281727   -0.206099\n",
      "281728    2.066971\n",
      "281729    0.509005\n",
      "Name: total_read_time, Length: 281730, dtype: float64)\n",
      "('sentiment_score', 0         0.711161\n",
      "1         0.587349\n",
      "2         0.598198\n",
      "3        -0.491223\n",
      "4         0.575861\n",
      "            ...   \n",
      "281725    0.310366\n",
      "281726   -0.175310\n",
      "281727   -0.379536\n",
      "281728    0.921770\n",
      "281729    0.409289\n",
      "Name: sentiment_score, Length: 281730, dtype: float64)\n",
      "('sentiment_label', 0         2\n",
      "1         1\n",
      "2         0\n",
      "3         2\n",
      "4         1\n",
      "         ..\n",
      "281725    2\n",
      "281726    1\n",
      "281727    1\n",
      "281728    0\n",
      "281729    0\n",
      "Name: sentiment_label, Length: 281730, dtype: int32)\n",
      "('last_modified_hour', 0          6\n",
      "1          6\n",
      "2         12\n",
      "3          6\n",
      "4          6\n",
      "          ..\n",
      "281725     6\n",
      "281726     6\n",
      "281727     6\n",
      "281728     6\n",
      "281729     6\n",
      "Name: last_modified_hour, Length: 281730, dtype: int32)\n",
      "('last_modified_day', 0         29\n",
      "1         29\n",
      "2          4\n",
      "3         29\n",
      "4         29\n",
      "          ..\n",
      "281725    29\n",
      "281726    29\n",
      "281727    29\n",
      "281728    29\n",
      "281729    29\n",
      "Name: last_modified_day, Length: 281730, dtype: int32)\n",
      "('published_time_hour', 0         19\n",
      "1         20\n",
      "2          4\n",
      "3         20\n",
      "4         19\n",
      "          ..\n",
      "281725     7\n",
      "281726     7\n",
      "281727    10\n",
      "281728    10\n",
      "281729    11\n",
      "Name: published_time_hour, Length: 281730, dtype: int32)\n",
      "('published_time_day', 0         21\n",
      "1         21\n",
      "2         21\n",
      "3         21\n",
      "4         21\n",
      "          ..\n",
      "281725    17\n",
      "281726     3\n",
      "281727    18\n",
      "281728    18\n",
      "281729    19\n",
      "Name: published_time_day, Length: 281730, dtype: int32)\n",
      "('clicked', 0         False\n",
      "1         False\n",
      "2         False\n",
      "3         False\n",
      "4         False\n",
      "          ...  \n",
      "281725    False\n",
      "281726    False\n",
      "281727    False\n",
      "281728     True\n",
      "281729    False\n",
      "Name: clicked, Length: 281730, dtype: bool)\n"
     ]
    }
   ],
   "source": [
    "X_train_phase1.loc[X_train_phase1[\"clicked\"] == False, \"read_time\"] = 0 #Reset reading times for unclicked articles\n",
    "X_train_phase1.loc[X_train_phase1[\"clicked\"] == False, \"scroll_percentage\"] = 0\n",
    "X_val_phase1.loc[X_val_phase1[\"clicked\"] == False, \"read_time\"] = 0 \n",
    "X_val_phase1.loc[X_val_phase1[\"clicked\"] == False, \"scroll_percentage\"] = 0\n",
    "\n",
    "X_train_phase1['scroll_percentage'].fillna(value=0, inplace=True) #Reset dataset with NaN values ​​to 0\n",
    "X_val_phase1['scroll_percentage'].fillna(value=0, inplace=True)\n",
    "X_train_phase1['total_inviews'].fillna(value=0, inplace=True)\n",
    "X_val_phase1['total_inviews'].fillna(value=0, inplace=True)\n",
    "X_train_phase1['total_pageviews'].fillna(value=0, inplace=True)\n",
    "X_val_phase1['total_pageviews'].fillna(value=0, inplace=True)\n",
    "X_train_phase1['total_read_time'].fillna(value=0, inplace=True)\n",
    "X_val_phase1['total_read_time'].fillna(value=0, inplace=True)\n",
    "\n",
    "X_history_final['scroll_percentage'].fillna(value=0, inplace=True)\n",
    "X_history_final['total_inviews'].fillna(value=0, inplace=True)\n",
    "X_history_final['total_pageviews'].fillna(value=0, inplace=True)\n",
    "X_history_final['total_read_time'].fillna(value=0, inplace=True)\n",
    "X_history_final['read_time'].fillna(value=0, inplace=True)\n",
    "\n",
    "Y_train_phase1['age'].fillna(value=0, inplace=True)\n",
    "Y_val_phase1['age'].fillna(value=0, inplace=True)\n",
    "Y_train_phase1['postcode'].fillna(value=0, inplace=True)\n",
    "Y_val_phase1['postcode'].fillna(value=0, inplace=True)\n",
    "\n",
    "Y_train_phase1[['age']] = Y_train_phase1[['age']].astype(int) #Since classification cannot be done with float, it is converted to int\n",
    "Y_val_phase1[['age']] = Y_val_phase1[['age']].astype(int)\n",
    "Y_train_phase1[['postcode']] = Y_train_phase1[['postcode']].astype(int)\n",
    "Y_val_phase1[['postcode']] = Y_val_phase1[['postcode']].astype(int)\n",
    "\n",
    "print(\"-------------------------------------------------\") \n",
    "\n",
    "for column in X_train_phase1.items():\n",
    "    print(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdd7d8e6-f74a-49a3-bd90-db38136ba832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Behavior Title 0 infinite values\n",
      "Behavior Subtitle 0 infinite values\n",
      "Behavior Ner_clusters 0 infinite values\n",
      "Behavior Entity_groups 0 infinite values\n",
      "Behavior Topics 0 infinite values\n",
      "History Title 0 infinite values\n",
      "History Subtitle 0 infinite values\n",
      "History Ner_clusters 0 infinite values\n",
      "History Entity_groups 0 infinite values\n",
      "History Topics 0 infinite values\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------------------------------\")   \n",
    "c = np.isinf(X_train_phase1['title']).values.sum() \n",
    "print(\"Behavior Title \" + str(c) + \" infinite values\") \n",
    "c = np.isinf(X_train_phase1['subtitle']).values.sum() \n",
    "print(\"Behavior Subtitle \" + str(c) + \" infinite values\") \n",
    "c = np.isinf(X_train_phase1['ner_clusters']).values.sum() \n",
    "print(\"Behavior Ner_clusters \" + str(c) + \" infinite values\") \n",
    "c = np.isinf(X_train_phase1['entity_groups']).values.sum() \n",
    "print(\"Behavior Entity_groups \" + str(c) + \" infinite values\") \n",
    "c = np.isinf(X_train_phase1['topics']).values.sum() \n",
    "print(\"Behavior Topics \" + str(c) + \" infinite values\")\n",
    "\n",
    "c = np.isinf(X_history_final['title']).values.sum() \n",
    "print(\"History Title \" + str(c) + \" infinite values\") \n",
    "c = np.isinf(X_history_final['subtitle']).values.sum() \n",
    "print(\"History Subtitle \" + str(c) + \" infinite values\") \n",
    "c = np.isinf(X_history_final['ner_clusters']).values.sum() \n",
    "print(\"History Ner_clusters \" + str(c) + \" infinite values\") \n",
    "c = np.isinf(X_history_final['entity_groups']).values.sum() \n",
    "print(\"History Entity_groups \" + str(c) + \" infinite values\") \n",
    "c = np.isinf(X_history_final['topics']).values.sum() \n",
    "print(\"History Topics \" + str(c) + \" infinite values\") \n",
    "\n",
    "print(\"-------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7684e4d-bf7d-48cd-b15d-a55572e72ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1 model training... \n",
      "Phase 1 prediction error: 0.12114341891002321\n",
      "Phase 1 model predicting... \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "print(\"Phase 1 model training... \")\n",
    "model_phase1 = MultiOutputRegressor(LogisticRegression(max_iter=2000)) \n",
    "#max_iter can be increased when an insufficient warning is received, but it consumes more resources.\n",
    "\n",
    "model_phase1.fit(X_train_phase1, Y_train_phase1) \n",
    "\n",
    "y_pred = model_phase1.predict(X_val_phase1)\n",
    "y_val = np.array(Y_val_phase1, dtype=int)\n",
    "\n",
    "y_pred_T = np.transpose(y_pred)\n",
    "\n",
    "error = np.mean( y_val != y_pred )\n",
    "print(\"Phase 1 prediction error: \"+str(error)) ##accuracy_score() and roc_auc_score() methods cannot be used because the output contains more than one value\n",
    "\n",
    "print(\"Phase 1 model predicting... \")\n",
    "X_combined_phase1 = pd.concat([X_train_phase1, X_val_phase1]) ##When you see that the model works, train and val tables are combined so that you can learn better.\n",
    "Y_combined_phase1 = pd.concat([Y_train_phase1, Y_val_phase1])\n",
    "\n",
    "model_phase1.fit(X_combined_phase1, Y_combined_phase1)\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train_phase1_scaled = scaler.fit_transform(X_train_phase1)\n",
    "X_val_phase1_scaled = scaler.transform(X_val_phase1)\n",
    "X_combined_phase1_scaled = scaler.fit_transform(pd.concat([X_train_phase1, X_val_phase1]))\n",
    "\n",
    "# We convert it back to DataFrame to keep feature names\n",
    "X_train_phase1_scaled_df = pd.DataFrame(X_train_phase1_scaled, columns=X_train_phase1.columns)\n",
    "X_val_phase1_scaled_df = pd.DataFrame(X_val_phase1_scaled, columns=X_val_phase1.columns)\n",
    "X_combined_phase1_scaled_df = pd.DataFrame(X_combined_phase1_scaled, columns=pd.concat([X_train_phase1, X_val_phase1]).columns)\n",
    "\n",
    "print(\"Phase 1 model training... \")\n",
    "\n",
    "model_phase1 = MultiOutputRegressor(LogisticRegression(max_iter=2000, solver='saga')) \n",
    "model_phase1.fit(X_train_phase1_scaled_df, Y_train_phase1)\n",
    "\n",
    "y_pred = model_phase1.predict(X_val_phase1_scaled_df)\n",
    "y_val = np.array(Y_val_phase1, dtype=int)\n",
    "\n",
    "y_pred_T = np.transpose(y_pred)\n",
    "\n",
    "error = np.mean(y_val != y_pred)\n",
    "print(\"Phase 1 prediction error: \" + str(error))\n",
    "\n",
    "print(\"Phase 1 model predicting... \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1e34031-acf8-402d-8c82-083054acb3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined_phase1 = pd.concat([X_train_phase1, X_val_phase1])\n",
    "Y_combined_phase1 = pd.concat([Y_train_phase1, Y_val_phase1])\n",
    "\n",
    "model_phase1.fit(X_combined_phase1_scaled_df, Y_combined_phase1)\n",
    "y_pred = model_phase1.predict(X_history_final) \n",
    "\n",
    "y_pred_T = np.transpose(y_pred)\n",
    "Y_history_final = pd.DataFrame() \n",
    "Y_history_final['device_type'] = y_pred_T[0]\n",
    "Y_history_final['is_sso_user'] = y_pred_T[1].astype('b')\n",
    "Y_history_final['gender'] = y_pred_T[2]\n",
    "Y_history_final['postcode'] = y_pred_T[3]\n",
    "Y_history_final['age'] = y_pred_T[4]\n",
    "Y_history_final['is_subscriber'] = y_pred_T[5].astype('b')\n",
    "\n",
    "X_total = pd.concat([X_history_final, X_combined_phase1]) # In the next prediction we will only look at whether the article was clicked or not\n",
    "Y_total = pd.concat([Y_history_final, Y_combined_phase1]) # Therefore, while all columns are included in table X, only ['clicked'] should be in table Y.\n",
    "\n",
    "X_total = X_total.reset_index(drop=True)\n",
    "Y_total = Y_total.reset_index(drop=True)\n",
    "\n",
    "X_total['device_type'] = Y_total['device_type']\n",
    "X_total['is_sso_user'] = Y_total['is_sso_user']\n",
    "X_total['gender'] = Y_total['gender']\n",
    "X_total['postcode'] = Y_total['postcode']\n",
    "X_total['age'] = Y_total['age']\n",
    "X_total['is_subscriber'] = Y_total['is_subscriber']\n",
    "\n",
    "Y_total = X_total.filter(['clicked'])\n",
    "X_total = X_total.drop(['clicked'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "315d5163-7a45-4a1f-b9d3-5ab94d86449f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty history columns are predicted. \n",
      "---------------------------\n",
      "Phase 2 model training... \n"
     ]
    }
   ],
   "source": [
    "X_train_phase2, X_val_phase2, Y_train_phase2, Y_val_phase2 = train_test_split(X_total, Y_total, test_size=0.2, random_state=42)\n",
    "\n",
    "X_history_final = X_history_final.reset_index()\n",
    "Y_history_final = Y_history_final.reset_index()\n",
    "\n",
    "X_history_final['device_type'] = Y_history_final['device_type']\n",
    "X_history_final['is_sso_user'] = Y_history_final['is_sso_user']\n",
    "X_history_final['gender'] = Y_history_final['gender']\n",
    "X_history_final['postcode'] = Y_history_final['postcode']\n",
    "X_history_final['age'] = Y_history_final['age']\n",
    "X_history_final['is_subscriber'] = Y_history_final['is_subscriber']\n",
    "\n",
    "Y_history_final = X_history_final.filter(['clicked'])\n",
    "X_history_final = X_history_final.drop(['clicked'], axis=1)\n",
    "\n",
    "X_history_final = X_history_final.drop(['index'], axis=1)\n",
    "\n",
    "test_history_final_to_compare = X_history_final\n",
    "\n",
    "print(\"Empty history columns are predicted. \")\n",
    "print(\"---------------------------\") \n",
    "print(\"Phase 2 model training... \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5b1b140-bf4d-4bc8-a235-371bf7ff2439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 2 prediction error: 0.002136987812275961\n",
      "Phase 2 model predicting... \n",
      "Saving the models... \n",
      "Model training completed. You can now predict on the test set.\n",
      "Predicting for final accuracy score...\n"
     ]
    }
   ],
   "source": [
    "scaler_phase2 = StandardScaler()\n",
    "X_train_phase2_scaled = scaler_phase2.fit_transform(X_train_phase2)\n",
    "X_val_phase2_scaled = scaler_phase2.transform(X_val_phase2)\n",
    "\n",
    "## Phase 2 model training\n",
    "model_phase2 = LogisticRegression(max_iter=1000, solver='saga')\n",
    "model_phase2.fit(X_train_phase2_scaled, Y_train_phase2.values.ravel())\n",
    "\n",
    "y_pred = model_phase2.predict(X_val_phase2_scaled)\n",
    "y_val = np.array(Y_val_phase2, dtype=int)\n",
    "y_val_formatted = list(chain(*y_val))\n",
    "\n",
    "error = np.mean(y_val_formatted != y_pred)\n",
    "print(\"Phase 2 prediction error: \" + str(error))\n",
    "\n",
    "print(\"Phase 2 model predicting... \")\n",
    "\n",
    "X_combined_phase2 = pd.concat([X_train_phase2, X_val_phase2])\n",
    "Y_combined_phase2 = pd.concat([Y_train_phase2, Y_val_phase2])\n",
    "\n",
    "X_combined_phase2_scaled = scaler_phase2.fit_transform(X_combined_phase2)\n",
    "\n",
    "model_phase2.fit(X_combined_phase2_scaled, Y_combined_phase2.values.ravel())\n",
    "\n",
    "# Modelleri kaydetme\n",
    "print(\"Saving the models... \")\n",
    "with open('model_phase1.sav', 'wb') as f:\n",
    "    pickle.dump(model_phase1, f)\n",
    "    \n",
    "with open('model_phase2.sav', 'wb') as f:\n",
    "    pickle.dump(model_phase2, f)\n",
    "\n",
    "print(\"Model training completed. You can now predict on the test set.\")\n",
    "print(\"Predicting for final accuracy score...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2f7e19b-5e3b-4438-914b-3e1dc3905a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for final accuracy score...\n",
      "AUC Score: 0.9785555629320918\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicting for final accuracy score...\")\n",
    "\n",
    "# Behaviors table not merged with History set is required to calculate final AUC scores\n",
    "X_combined_phase1 = pd.concat([X_train_phase1, X_val_phase1])\n",
    "Y_combined_phase1 = pd.concat([Y_train_phase1, Y_val_phase1])\n",
    "\n",
    "X_combined_phase1 = X_combined_phase1.reset_index()\n",
    "Y_combined_phase1 = Y_combined_phase1.reset_index()\n",
    "\n",
    "ground_truth = pd.DataFrame()\n",
    "ground_truth['clicked'] = X_combined_phase1['clicked'] # Gerçek tıklanma değerleri\n",
    "X_combined_phase1 = X_combined_phase1.drop(['clicked'], axis=1)\n",
    "\n",
    "X_combined_phase1['device_type'] = Y_combined_phase1['device_type']\n",
    "X_combined_phase1['is_sso_user'] = Y_combined_phase1['is_sso_user']\n",
    "X_combined_phase1['gender'] = Y_combined_phase1['gender']\n",
    "X_combined_phase1['postcode'] = Y_combined_phase1['postcode']\n",
    "X_combined_phase1['age'] = Y_combined_phase1['age']\n",
    "X_combined_phase1['is_subscriber'] = Y_combined_phase1['is_subscriber']\n",
    "\n",
    "X_combined_phase1 = X_combined_phase1.drop(['index'], axis=1)\n",
    "Y_combined_phase1 = Y_combined_phase1.drop(['index'], axis=1)\n",
    "\n",
    "X_combined_phase1_scaled = scaler_phase2.transform(X_combined_phase1)\n",
    "\n",
    "y_pred = model_phase2.predict(X_combined_phase1_scaled) # Karşılaştırma yapılacağı için history ile birleştirilmemeli o yüzden phase 1 dataseti alıyoruz\n",
    "\n",
    "# We can calculate AUC score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc_score = roc_auc_score(ground_truth, y_pred)\n",
    "print(\"AUC Score: \" + str(auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9966678f-b259-4ab1-a489-92f908700523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score calculation...\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "Y_final = pd.DataFrame() \n",
    "Y_final['clicked'] = y_pred\n",
    "\n",
    "X_final = X_combined_phase1\n",
    "X_final['clicked'] = Y_final['clicked'] \n",
    "X_final['impression_id'] = final_ids['impression_id']\n",
    "X_final['article_id'] = final_ids['article_id']\n",
    "\n",
    "\n",
    "print(\"Score calculation...\") \n",
    "\n",
    "ground_truth['impression_id'] = X_final['impression_id']\n",
    "\n",
    "current_impression = pd.DataFrame()\n",
    "current_impression_y_truth = pd.DataFrame()\n",
    "user_previous_history = pd.DataFrame()\n",
    "\n",
    "total_roc_auc_score_list = []\n",
    "\n",
    "print (\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e036f29-ea2a-4e43-8708-a32ef5a69c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = 0\n",
    "while i < X_final[X_final.columns[0]].count():    \n",
    "    current_impression = X_final.loc[X_final['impression_id'] == X_final['impression_id'][i]]\n",
    "    current_impression_y_truth = ground_truth.loc[ground_truth['impression_id'] == ground_truth['impression_id'][i]]\n",
    "    user_previous_history = test_history_final_to_compare.loc[test_history_final_to_compare['user_id'] == current_impression['user_id'].iloc[0]]\n",
    "    \n",
    "    current_impression = current_impression.reset_index(drop=True)\n",
    "    current_impression_y_truth = current_impression_y_truth.reset_index(drop=True)\n",
    "    \n",
    "    current_impression_y_truth['truth'] = np.where((current_impression_y_truth['clicked'] == True), 1, 0)\n",
    "    \n",
    "    current_impression['rank'] = np.where((current_impression['clicked'] == True), 0, 10).astype(float)\n",
    "    \n",
    "    for j in range(current_impression[current_impression.columns[0]].count()):\n",
    "        distance_score = 0\n",
    "        distance_score += mean(current_impression['premium'].iloc[j] != user_previous_history['premium'])\n",
    "        distance_score += mean(abs(current_impression['image_ids'].iloc[j] - user_previous_history['image_ids']))\n",
    "        distance_score += mean(abs(current_impression['category'].iloc[j] - user_previous_history['category']))\n",
    "        distance_score += mean(abs(current_impression['subcategory'].iloc[j] - user_previous_history['subcategory']))\n",
    "        distance_score += mean(abs(current_impression['total_inviews'].iloc[j] - user_previous_history['total_inviews']))\n",
    "        distance_score += mean(abs(current_impression['total_pageviews'].iloc[j] - user_previous_history['total_pageviews']))\n",
    "        distance_score += mean(abs(current_impression['total_read_time'].iloc[j] - user_previous_history['total_read_time']))\n",
    "        distance_score += mean(abs(current_impression['sentiment_score'].iloc[j] - user_previous_history['sentiment_score']))\n",
    "        distance_score += mean(abs(current_impression['article_type'].iloc[j] - user_previous_history['article_type']))\n",
    "        distance_score += (mean(abs(current_impression['category_str'].iloc[j] - user_previous_history['category_str'])) / 5)\n",
    "        distance_score += mean(abs(current_impression['sentiment_label'].iloc[j] - user_previous_history['sentiment_label']))\n",
    "        \n",
    "        current_impression['rank'].iloc[j] = round(current_impression['rank'].iloc[j] + distance_score, 5)\n",
    "    \n",
    "    current_impression['rank'] = current_impression['rank'].rank()\n",
    "    current_impression['rank'] = round(1 / current_impression['rank'], 5)\n",
    "    \n",
    "    y_pred = current_impression['rank'].tolist()\n",
    "    y_true = current_impression_y_truth['truth'].tolist()\n",
    "    \n",
    "    total_roc_auc_score_list.append(roc_auc_score(y_true, y_pred))\n",
    "    \n",
    "    i = i + current_impression[current_impression.columns[0]].count()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f036e7a-aec3-4b47-868a-cc31deeb3d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Average AUC score across all impressions: 0.9698444749253172\n"
     ]
    }
   ],
   "source": [
    "print(\"----------\")\n",
    "print(\"Average AUC score across all impressions: \" + str(mean(total_roc_auc_score_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "03678c74-f4f4-4c41-bfe9-2275739b4ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "34e9a6d9-cd6b-4b23-94f9-05d2ca9f709d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3696ec2b-e4be-443e-add5-eef4dce8ada8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
